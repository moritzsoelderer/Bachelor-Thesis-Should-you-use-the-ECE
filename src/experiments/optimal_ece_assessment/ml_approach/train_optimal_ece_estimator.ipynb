{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import keras_tuner\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow.keras as keras\n",
    "from keras.src.layers import Dropout\n",
    "from keras.src.layers import Dense\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from src.metrics.ece import ece"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def interpolate_indexed_value_array(indexing_array: np.ndarray, value_array: np.ndarray, searched_for, strategy=\"linear\"):\n",
    "    f = interp1d(indexing_array, value_array, kind=strategy,\n",
    "                 bounds_error=False, fill_value=(value_array[0], value_array[-1]))\n",
    "    return f(searched_for)\n",
    "\n",
    "\n",
    "def safe_unpickle_all(file):\n",
    "    objects = []\n",
    "    unpickler = pickle.Unpickler(file)\n",
    "    while True:\n",
    "        try:\n",
    "            obj = unpickler.load()\n",
    "            objects.append(obj)\n",
    "        except EOFError:\n",
    "            break  # End of file (incomplete last object)\n",
    "        except Exception as e:\n",
    "            print(\"Partial data recovered. Stopped at error:\", e)\n",
    "            break\n",
    "    return objects\n",
    "\n",
    "def build_nn(hp) -> keras.Model:\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Flatten())\n",
    "    for i in range(hp.Int('num_layers', 5, 14)):\n",
    "        model.add(\n",
    "            Dense(\n",
    "                units=hp.Int(f\"units_{i}\", min_value=256, max_value=1536, step=32),\n",
    "                activation=hp.Choice(f\"activation_{i}\", [\"relu\", \"linear\"]),\n",
    "            )\n",
    "        )\n",
    "    model.add(Dense(1, activation=\"relu\"))\n",
    "    model.add(Dropout(hp.Float(\"Dropout\", 0.05, 0.25, sampling='log')))\n",
    "\n",
    "    optimizer_choice = hp.Choice('optimizer', ['adam', 'rmsprop'])\n",
    "    learning_rate = hp.Float('learning_rate', 1e-5, 5e-4, sampling='log')\n",
    "\n",
    "    optimizer = None\n",
    "    if optimizer_choice == 'adam':\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_choice == 'sgd':\n",
    "        optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    elif optimizer_choice == 'rmsprop':\n",
    "        optimizer = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.LogCosh,\n",
    "        metrics=[\"mae\", \"mse\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def get_rounded_clipped_sample_size(sample_size):\n",
    "    return max(15, min(10000, round(sample_size)))\n",
    "\n",
    "def calculate_ece_on_subsets(p_pred, y_true, sample_size, num=10):\n",
    "    eces = []\n",
    "    for i in range(num):\n",
    "        indices = np.random.choice(p_pred.shape[0], size=get_rounded_clipped_sample_size(sample_size), replace=True)\n",
    "        p_pred_subset = p_pred[indices]\n",
    "        y_true_subset = y_true[indices]\n",
    "        eces.append(ece(p_pred_subset, y_true_subset, 15))\n",
    "    return np.array(eces)\n",
    "        "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca896e1b5adf9bf6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Gather Data from different files\n",
    "print(\"Gathering data...\")\n",
    "data = []\n",
    "dirs = [\"./data/20250524_024937\", \"./data/20250524_140927\"]\n",
    "\n",
    "for dir in dirs:\n",
    "    files = Path(dir).glob(\"*.pkl\")\n",
    "    for file_name in files:\n",
    "        print(file_name, sep=\", \")\n",
    "        with (open(f'{file_name}', 'rb') as file):\n",
    "            try:\n",
    "                content = pickle.load(file)\n",
    "            except EOFError:\n",
    "                print(\"Data is corrupted. Trying to partially recover data...\")\n",
    "                content = safe_unpickle_all(file)\n",
    "                print(content)\n",
    "\n",
    "            data = data + content"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "51949d32f8799ddd",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_results = []\n",
    "y_tests = []\n",
    "p_tests = []\n",
    "eces = []\n",
    "sample_sizes = []\n",
    "accuracies = []\n",
    "y = []\n",
    "optimal_eces = []\n",
    "for result in data:\n",
    "    model_result = result[\"model_results\"]\n",
    "    model_results = model_results + model_result\n",
    "    \n",
    "    for r in model_result:\n",
    "        p_tests.append(r[\"p_test\"])\n",
    "        eces.append(r[\"ECEs\"])\n",
    "        accuracies.append(r[\"Accuracy\"])\n",
    "        y.append(r[\"Optimal Sample Size\"])\n",
    "        optimal_eces.append(r[\"Optimal ECE\"])\n",
    "\n",
    "    for i in range(4):\n",
    "        y_tests.append(result[\"y_test\"])\n",
    "        sample_sizes.append(result[\"Sample Sizes\"])\n",
    "\n",
    "# Ensure numpy\n",
    "y_tests = np.array(y_tests)\n",
    "eces = np.array(eces)\n",
    "p_tests = np.array(p_tests, dtype=np.float32)\n",
    "sample_sizes = np.array(sample_sizes)\n",
    "accuracies = np.array(accuracies)\n",
    "y = np.array(y)\n",
    "optimal_eces = np.array(optimal_eces)\n",
    "\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "std_dev_accuracy = np.std(accuracies)\n",
    "\n",
    "dist_from_eces0 = np.array([np.linalg.norm(eces[0] - eces[i]) for i in range(len(eces)) if i != 0])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6723eff4e6e3d7b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(\"y_tests shape\", y_tests.shape)\n",
    "print(\"sample sizes shape\", sample_sizes.shape)\n",
    "print(\"p_tests shape\", p_tests.shape)\n",
    "print(\"eces shape\", eces.shape)\n",
    "print(\"Mean Accuracy\", mean_accuracy)\n",
    "print(\"Std. Dev. Accuracy\", std_dev_accuracy)\n",
    "print(\"Distances from eces[0]\", dist_from_eces0)\n",
    "print(\"Mean distance from eces[0]\", np.mean(dist_from_eces0))\n",
    "print(\"Std. Dev distance from eces[0]\", np.std(dist_from_eces0))\n",
    "\n",
    "print(\"Mean True ECE\", np.mean(np.array([result[\"True ECE Dists (Binned - 15 Bins)\"] for result in model_results])))\n",
    "print(\"Std. Dev True ECE\", np.std(np.array([result[\"True ECE Dists (Binned - 15 Bins)\"] for result in model_results])))\n",
    "\n",
    "print(sample_sizes)\n",
    "print(eces)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a5cc628353721db",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Prepare Data\n",
    "X = np.hstack((sample_sizes, eces, accuracies.reshape(-1, 1)))\n",
    "y = np.column_stack((y, optimal_eces))\n",
    "\n",
    "print(X)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4bcce1c39a507fbf",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "optimal_eces_train = y_train[:, 1]\n",
    "y_train = y_train[:, 0]\n",
    "optimal_eces_test = y_test[:, 1]\n",
    "y_test = y_test[:, 0]\n",
    "\n",
    "X_train = X_train[:, 100:]\n",
    "X_test = X_test[:, 100:]\n",
    "\n",
    "print(\"X_train Shape\", X_train.shape)\n",
    "print(\"X_test Shape\", X_test.shape)\n",
    "\n",
    "print(\"y_train Shape\", y_train.shape)\n",
    "print(\"y_test Shape\", y_test.shape)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b4b4ff5652a94f31",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Perform Bayes Optimization\n",
    "tuner = keras_tuner.BayesianOptimization(\n",
    "    hypermodel=build_nn,\n",
    "    objective=\"val_loss\",\n",
    "    max_trials=24,\n",
    "    executions_per_trial=1,\n",
    "    directory=\"keras_tuner_logs_bayes_logcosh_dropout_only_ece_values_ext\",\n",
    "    project_name=\"ece_neural_network_bayes2_logscosh_dropout_only_ece_values_ext\",\n",
    ")\n",
    "tuner.search(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=150,\n",
    "    batch_size=128,\n",
    "    callbacks=[keras.callbacks.EarlyStopping(patience=10)],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "y_pred_scaled = np.array(best_model.predict(X_test)).flatten()\n",
    "y_pred = y_pred_scaled\n",
    "print(\"y_pred_scaled NN\", y_pred_scaled)\n",
    "\n",
    "# Print best Hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"Best Hyperparameters:\")\n",
    "for key in best_hps.values.keys():\n",
    "    print(f\"{key}: {best_hps.get(key)}\")\n",
    "\n",
    "\"\"\"\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten())\n",
    "for i in range(4):\n",
    "    model.add(\n",
    "        Dense(\n",
    "            units=480,\n",
    "            activation=\"relu\",\n",
    "        )\n",
    "    )\n",
    "model.add(Dense(1, activation=\"linear\"))\n",
    "\n",
    "learning_rate = 0.0022035143136497045\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    loss=keras.losses.LogCosh(),\n",
    "    metrics=[\"mae\"],\n",
    ")\n",
    "\n",
    "model.fit(X_train_scaled, y_train_scaled)\n",
    "y_pred = y_scaler.inverse_transform(np.array(model.predict(X_test_scaled)).flatten().reshape(-1, 1)).ravel()\n",
    "\"\"\"\n",
    "\n",
    "print(\"y_pred NN\", y_pred)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "626215944816b53d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Train Regressor\n",
    "model = XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    max_depth=16,\n",
    "    learning_rate=0.002,\n",
    "    subsample=0.78,\n",
    "    colsample_bytree=0.75,\n",
    "    gamma=1,\n",
    "    reg_alpha=0.15,\n",
    "    reg_lambda=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbosity=1\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "y_pred_scaled_regressor = np.array(model.predict(X_test)).flatten()\n",
    "y_pred_regressor = y_pred_scaled_regressor\n",
    "print(\"y_pred_scaled regressor\", y_pred_scaled_regressor)\n",
    "print(\"y_pred_regressor\", y_pred_regressor)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "425921406125bf1c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Calculate Metrics and display Dataframe\n",
    "print(\"X_test\", X_test)\n",
    "X_test_eces = X_test[:, :100]\n",
    "print(\"X_test_eces\", X_test_eces)\n",
    "print(\"Sample Sizes\", sample_sizes)\n",
    "\n",
    "\n",
    "simple_strategy_preds = [\n",
    "    (f\"{(i + 1) * 100} Samples\", (sample_sizes[len(X_train):, i], X_test_eces[:, i])) for i in [0, 4, 9, 19, 49, 79, 99]]\n",
    "\n",
    "preds = {}\n",
    "for pred in simple_strategy_preds:\n",
    "    preds[pred[0]] = pred[1]\n",
    "    \n",
    "\n",
    "num_ece_subsets = 100\n",
    "\n",
    "nn_ece_on_subsets = []\n",
    "regressor_ece_on_subsets = []\n",
    "for i, sample_size_nn in enumerate(y_pred):\n",
    "    print(f\"Prediction number {i} of {len(y_pred)}\")\n",
    "    nn_ece_on_subsets.append(np.mean(calculate_ece_on_subsets(p_tests[i], y_tests[i], sample_size_nn, num_ece_subsets)))\n",
    "    regressor_ece_on_subsets.append(np.mean(calculate_ece_on_subsets(p_tests[i], y_tests[i], y_pred_regressor[i], num_ece_subsets)))\n",
    "\n",
    "nn_ece_on_subsets = np.array(nn_ece_on_subsets)\n",
    "regressor_ece_on_subsets = np.array(regressor_ece_on_subsets)\n",
    "\n",
    "ece_nn_interpolated = np.array([interpolate_indexed_value_array(sample_sizes[i], X_test_eces[i], sample_size) for\n",
    "                                (i, sample_size) in enumerate(y_pred)])\n",
    "\n",
    "\n",
    "ece_regressor_interpolated = np.array([interpolate_indexed_value_array(sample_sizes[i], X_test_eces[i], sample_size) for\n",
    "                      (i, sample_size) in enumerate(y_pred_regressor)])\n",
    "\n",
    "ece_nn_interpolated_quad = np.array([interpolate_indexed_value_array(sample_sizes[i], X_test_eces[i], sample_size, strategy=\"quadratic\") for\n",
    "                      (i, sample_size) in enumerate(y_pred)])\n",
    "\n",
    "ece_regressor_interpolated_quad = np.array([interpolate_indexed_value_array(sample_sizes[i], X_test_eces[i], sample_size, strategy=\"quadratic\") for\n",
    "                                      (i, sample_size) in enumerate(y_pred_regressor)])\n",
    "preds.update({\n",
    "    \"Neural Network\": (\n",
    "        y_pred, np.array([ece(p_tests[i][:get_rounded_clipped_sample_size(sample_size)], y_tests[i][:get_rounded_clipped_sample_size(sample_size)], n_bins=15) for\n",
    "                          i, sample_size in enumerate(y_pred)])),\n",
    "    \"XGBRegressor\": (y_pred_regressor,\n",
    "                     np.array([ece(p_tests[i][:get_rounded_clipped_sample_size(sample_size)], y_tests[i][:get_rounded_clipped_sample_size(sample_size)], n_bins=15) for\n",
    "                               i, sample_size in enumerate(y_pred_regressor)])),\n",
    "    f\"Neural Network (ECE on subsets {num_ece_subsets})\": (\n",
    "        y_pred, nn_ece_on_subsets),\n",
    "    f\"XGBRegressor (ECE on subsets {num_ece_subsets})\": (\n",
    "        y_pred_regressor, regressor_ece_on_subsets),\n",
    "    \"Neural Network (Interpolated)\": (y_pred, ece_nn_interpolated),\n",
    "    \"XGBRegressor (Interpolated)\": (y_pred_regressor, ece_regressor_interpolated),\n",
    "    \"Interpolated Average\": ((y_pred + y_pred_regressor)/ 2, (ece_nn_interpolated + ece_regressor_interpolated) / 2),\n",
    "    \"Neural Network (Interpolated - Quadratic)\": (y_pred, ece_nn_interpolated_quad),\n",
    "    \"XGBRegressor (Interpolated - Quadratic)\": (y_pred_regressor, ece_regressor_interpolated_quad),\n",
    "    \"Interpolated (Quadratic) Average\": ((y_pred + y_pred_regressor) / 2, (ece_nn_interpolated_quad + ece_regressor_interpolated_quad) / 2),\n",
    "    \"Y_Test\": (y_test, optimal_eces_test)\n",
    "})\n",
    "\n",
    "print(\"Optimal ECEs\", optimal_eces_test)\n",
    "print(\"y_test\", y_test)\n",
    "# Compute stats\n",
    "results = []\n",
    "for name, (sample_sizes_, eces) in preds.items():\n",
    "    print(name, sample_sizes_, eces)\n",
    "    results.append({\n",
    "        \"Name\": name,\n",
    "        \"Mean (Samples)\": np.mean(sample_sizes_),\n",
    "        \"Std Dev (Samples)\": np.std(sample_sizes_),\n",
    "        \"MSE (Samples)\": mean_squared_error(y_test, sample_sizes_),\n",
    "        \"MAE (Samples)\": mean_absolute_error(y_test, sample_sizes_),\n",
    "        \"R2-Score (Samples)\": r2_score(y_test, sample_sizes_),\n",
    "        \"Mean (ECE)\": np.mean(eces),\n",
    "        \"Std Dev (ECE)\": np.std(eces),\n",
    "        \"MSE (ECE)\": mean_squared_error(optimal_eces_test, eces),\n",
    "        \"MAE (ECE)\": mean_absolute_error(optimal_eces_test, eces),\n",
    "        \"R2-Score (ECE)\": r2_score(optimal_eces_test, eces),\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "pd.set_option('display.max_columns', None)\n",
    "df = pd.DataFrame(results)\n",
    "print(df) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0706e7769c701e7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(list(preds.keys()))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "329e47a821416bec",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "num_preds = len(preds)\n",
    "cols = 6 \n",
    "rows = int(np.ceil(num_preds / cols))\n",
    "\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(30, 4 * rows), constrained_layout=True)\n",
    "\n",
    "axs = axs.flatten()\n",
    "\n",
    "for i, (key, value) in enumerate(preds.items()):\n",
    "    ax = axs[i]\n",
    "    pred_sample_sizes = value[0]\n",
    "    bin_range = (min(y_test.min(), pred_sample_sizes.min()), max(y_test.max(), pred_sample_sizes.max()))\n",
    "    ax.hist(y_test, bins=50, range=bin_range, alpha=0.5, label=\"Optimal Sample Sizes\")\n",
    "    ax.hist(pred_sample_sizes, bins=50, range=bin_range, alpha=0.5, label=key)\n",
    "    ax.set_title(f\"{key} vs Optimal\")\n",
    "    ax.set_xlabel(\"Sample Sizes\")\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "    ax.legend()\n",
    "\n",
    "# Hide unused subplots if any\n",
    "for j in range(i + 1, len(axs)):\n",
    "    fig.delaxes(axs[j])\n",
    "\n",
    "plt.suptitle(\"Sample Sizes - Target vs Prediction Distributions\", fontsize=16)\n",
    "plt.savefig(\"./optimal_ece_estimator_sample_sizes_distribution_comparison\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0b807f6d4bf2eb3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "num_preds = len(preds)\n",
    "cols = 6\n",
    "rows = int(np.ceil(num_preds / cols))\n",
    "\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(30, 4 * rows), constrained_layout=True)\n",
    "\n",
    "axs = axs.flatten()\n",
    "\n",
    "for i, (key, value) in enumerate(preds.items()):\n",
    "    ax = axs[i]\n",
    "    pred_eces = value[1]\n",
    "    bin_range = (min(optimal_eces_test.min(), pred_eces.min()), max(optimal_eces_test.max(), pred_eces.max()))\n",
    "    ax.hist(optimal_eces_test, bins=50, range=bin_range, alpha=0.5, label=\"Optimal ECEs\")\n",
    "    ax.hist(pred_eces, bins=50, range=bin_range, alpha=0.5, label=key)\n",
    "    ax.set_title(f\"{key} vs Optimal\")\n",
    "    ax.set_xlabel(\"ECE\")\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "    ax.legend()\n",
    "\n",
    "for j in range(i + 1, len(axs)):\n",
    "    fig.delaxes(axs[j])\n",
    "\n",
    "plt.suptitle(\"ECE - Target vs Prediction Distributions\", fontsize=16)\n",
    "plt.savefig(\"./optimal_ece_estimator_ece_distribution_comparison\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7602d9848f69654",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "num_preds = len(preds)\n",
    "cols = 6\n",
    "rows = int(np.ceil(num_preds / cols))\n",
    "\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(30, 4 * rows), constrained_layout=True)\n",
    "axs = axs.flatten()\n",
    "\n",
    "for i, (key, value) in enumerate(preds.items()):\n",
    "    ax = axs[i]\n",
    "    if len(value) > 1:\n",
    "        ax.scatter(value[1], optimal_eces_test)\n",
    "        ax.plot(optimal_eces_test, optimal_eces_test, c='red')\n",
    "        ax.set_title(f\"{key} vs Optimal\")\n",
    "        ax.set_xlabel(\"Prediction\")\n",
    "        ax.set_ylabel(\"Optimal ECE\")\n",
    "\n",
    "for j in range(i + 1, len(axs)):\n",
    "    fig.delaxes(axs[j])\n",
    "\n",
    "plt.suptitle(\"Target vs Prediction Plot\", fontsize=16)\n",
    "plt.savefig(\"./optimal_ece_estimator_reliability_diagrams\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97733e6cf7ba5c44",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(y_test/ 15)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae2b98918154efa",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plt.title(\"Calculated vs. Interpolated ECE\")\n",
    "plt.bar(sample_sizes[0], preds[f\"Neural Network (ECE on subsets {num_ece_subsets})\"], bins=50, label=\"Neural Network (ECE on subsets {num_ece_subsets})\")\n",
    "plt.bar(sample_sizes[0], preds[\"Neural Network (Interpolated)\"], bins=50,\n",
    "         label=\"Neural Network (Interpolated)\")\n",
    "plt.xlabel(\"sample sizes\")\n",
    "plt.ylabel(\"ECE\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e84ad8b1cc702937",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "359aed9e4f196516"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
