{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-18T09:58:44.419125100Z",
     "start_time": "2025-05-18T09:58:21.521872600Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import keras_tuner\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow.keras as keras\n",
    "from keras.src.layers import Dropout\n",
    "from keras.src.layers import Dense\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from src.metrics.ece import ece"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[34minterpolate_indexed_value_array\u001B[39m(indexing_array: \u001B[43mnp\u001B[49m.ndarray, value_array: np.ndarray, searched_for, strategy=\u001B[33m\"\u001B[39m\u001B[33mlinear\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m      2\u001B[39m     f = interp1d(indexing_array, value_array, kind=strategy,\n\u001B[32m      3\u001B[39m                  bounds_error=\u001B[38;5;28;01mFalse\u001B[39;00m, fill_value=(value_array[\u001B[32m0\u001B[39m], value_array[-\u001B[32m1\u001B[39m]))\n\u001B[32m      4\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m f(searched_for)\n",
      "\u001B[31mNameError\u001B[39m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "def interpolate_indexed_value_array(indexing_array: np.ndarray, value_array: np.ndarray, searched_for, strategy=\"linear\"):\n",
    "    f = interp1d(indexing_array, value_array, kind=strategy,\n",
    "                 bounds_error=False, fill_value=(value_array[0], value_array[-1]))\n",
    "    return f(searched_for)\n",
    "\n",
    "\n",
    "def safe_unpickle_all(file):\n",
    "    objects = []\n",
    "    unpickler = pickle.Unpickler(file)\n",
    "    while True:\n",
    "        try:\n",
    "            obj = unpickler.load()\n",
    "            objects.append(obj)\n",
    "        except EOFError:\n",
    "            break  # End of file (incomplete last object)\n",
    "        except Exception as e:\n",
    "            print(\"Partial data recovered. Stopped at error:\", e)\n",
    "            break\n",
    "    return objects\n",
    "\n",
    "def build_nn(hp) -> keras.Model:\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Flatten())\n",
    "    for i in range(hp.Int('num_layers', 5, 14)):\n",
    "        model.add(\n",
    "            Dense(\n",
    "                units=hp.Int(f\"units_{i}\", min_value=256, max_value=1536, step=32),\n",
    "                activation=hp.Choice(f\"activation_{i}\", [\"relu\", \"linear\"]),\n",
    "            )\n",
    "        )\n",
    "    model.add(Dense(1, activation=\"relu\"))\n",
    "    model.add(Dropout(hp.Float(\"Dropout\", 0.05, 0.25, sampling='log')))\n",
    "\n",
    "    optimizer_choice = hp.Choice('optimizer', ['adam', 'rmsprop'])\n",
    "    learning_rate = hp.Float('learning_rate', 1e-5, 5e-4, sampling='log')\n",
    "\n",
    "    optimizer = None\n",
    "    if optimizer_choice == 'adam':\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_choice == 'sgd':\n",
    "        optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    elif optimizer_choice == 'rmsprop':\n",
    "        optimizer = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.LogCosh,\n",
    "        metrics=[\"mae\", \"mse\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def calculate_ece_on_subsets(p_pred, y_true, sample_size, num=10):\n",
    "    eces = []\n",
    "    for i in range(num):\n",
    "        indices = np.random.choice(p_pred, size=sample_size, replace=True)\n",
    "        p_pred_subset = p_pred[indices]\n",
    "        y_true_subset = y_true[indices]\n",
    "        eces.append(ece(p_pred_subset, y_true_subset, 15))\n",
    "    return np.array(eces)\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-21T12:23:42.447746300Z",
     "start_time": "2025-05-21T12:23:42.081312100Z"
    }
   },
   "id": "ca896e1b5adf9bf6",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering data...\n",
      "data\\20250512_165251\\batch_0.pkl\n",
      "data\\20250512_165251\\batch_1.pkl\n",
      "data\\20250512_165251\\batch_10.pkl\n",
      "data\\20250512_165251\\batch_11.pkl\n",
      "data\\20250512_165251\\batch_12.pkl\n",
      "data\\20250512_165251\\batch_13.pkl\n",
      "data\\20250512_165251\\batch_14.pkl\n",
      "data\\20250512_165251\\batch_15.pkl\n",
      "data\\20250512_165251\\batch_16.pkl\n",
      "data\\20250512_165251\\batch_17.pkl\n",
      "data\\20250512_165251\\batch_18.pkl\n",
      "data\\20250512_165251\\batch_19.pkl\n",
      "data\\20250512_165251\\batch_2.pkl\n",
      "data\\20250512_165251\\batch_20.pkl\n",
      "data\\20250512_165251\\batch_21.pkl\n",
      "data\\20250512_165251\\batch_22.pkl\n",
      "data\\20250512_165251\\batch_23.pkl\n",
      "data\\20250512_165251\\batch_24.pkl\n",
      "data\\20250512_165251\\batch_25.pkl\n",
      "data\\20250512_165251\\batch_26.pkl\n",
      "data\\20250512_165251\\batch_27.pkl\n",
      "data\\20250512_165251\\batch_28.pkl\n",
      "data\\20250512_165251\\batch_29.pkl\n",
      "data\\20250512_165251\\batch_3.pkl\n",
      "data\\20250512_165251\\batch_30.pkl\n",
      "data\\20250512_165251\\batch_31.pkl\n",
      "data\\20250512_165251\\batch_4.pkl\n",
      "data\\20250512_165251\\batch_5.pkl\n",
      "data\\20250512_165251\\batch_6.pkl\n",
      "data\\20250512_165251\\batch_7.pkl\n",
      "data\\20250512_165251\\batch_8.pkl\n",
      "data\\20250512_165251\\batch_9.pkl\n",
      "data\\20250511_095828\\batch_0.pkl\n",
      "data\\20250511_095828\\batch_1.pkl\n",
      "data\\20250511_095828\\batch_10.pkl\n",
      "data\\20250511_095828\\batch_11.pkl\n",
      "data\\20250511_095828\\batch_12.pkl\n",
      "data\\20250511_095828\\batch_13.pkl\n",
      "data\\20250511_095828\\batch_14.pkl\n",
      "data\\20250511_095828\\batch_15.pkl\n",
      "data\\20250511_095828\\batch_16.pkl\n",
      "data\\20250511_095828\\batch_17.pkl\n",
      "data\\20250511_095828\\batch_18.pkl\n",
      "data\\20250511_095828\\batch_19.pkl\n",
      "data\\20250511_095828\\batch_2.pkl\n",
      "data\\20250511_095828\\batch_20.pkl\n",
      "data\\20250511_095828\\batch_21.pkl\n",
      "data\\20250511_095828\\batch_22.pkl\n",
      "data\\20250511_095828\\batch_23.pkl\n",
      "data\\20250511_095828\\batch_24.pkl\n",
      "data\\20250511_095828\\batch_25.pkl\n",
      "data\\20250511_095828\\batch_26.pkl\n",
      "data\\20250511_095828\\batch_27.pkl\n",
      "data\\20250511_095828\\batch_28.pkl\n",
      "data\\20250511_095828\\batch_3.pkl\n",
      "data\\20250511_095828\\batch_4.pkl\n",
      "data\\20250511_095828\\batch_5.pkl\n",
      "data\\20250511_095828\\batch_6.pkl\n",
      "data\\20250511_095828\\batch_7.pkl\n",
      "data\\20250511_095828\\batch_8.pkl\n",
      "data\\20250511_095828\\batch_9.pkl\n",
      "data\\20250512_012245\\batch_0.pkl\n",
      "data\\20250512_012245\\batch_1.pkl\n",
      "data\\20250512_012245\\batch_10.pkl\n",
      "data\\20250512_012245\\batch_11.pkl\n",
      "data\\20250512_012245\\batch_12.pkl\n",
      "data\\20250512_012245\\batch_13.pkl\n",
      "data\\20250512_012245\\batch_14.pkl\n",
      "data\\20250512_012245\\batch_15.pkl\n",
      "data\\20250512_012245\\batch_16.pkl\n",
      "data\\20250512_012245\\batch_17.pkl\n",
      "data\\20250512_012245\\batch_18.pkl\n",
      "data\\20250512_012245\\batch_19.pkl\n",
      "data\\20250512_012245\\batch_2.pkl\n",
      "data\\20250512_012245\\batch_20.pkl\n",
      "data\\20250512_012245\\batch_21.pkl\n",
      "data\\20250512_012245\\batch_22.pkl\n",
      "data\\20250512_012245\\batch_23.pkl\n",
      "data\\20250512_012245\\batch_24.pkl\n",
      "data\\20250512_012245\\batch_25.pkl\n",
      "data\\20250512_012245\\batch_26.pkl\n",
      "data\\20250512_012245\\batch_27.pkl\n",
      "data\\20250512_012245\\batch_28.pkl\n",
      "data\\20250512_012245\\batch_29.pkl\n",
      "data\\20250512_012245\\batch_3.pkl\n",
      "data\\20250512_012245\\batch_4.pkl\n",
      "data\\20250512_012245\\batch_5.pkl\n",
      "data\\20250512_012245\\batch_6.pkl\n",
      "data\\20250512_012245\\batch_7.pkl\n",
      "data\\20250512_012245\\batch_8.pkl\n",
      "data\\20250512_012245\\batch_9.pkl\n",
      "data\\20250510_185507\\batch_0.pkl\n",
      "data\\20250510_185507\\batch_1.pkl\n",
      "data\\20250510_185507\\batch_10.pkl\n",
      "data\\20250510_185507\\batch_11.pkl\n",
      "data\\20250510_185507\\batch_12.pkl\n",
      "data\\20250510_185507\\batch_13.pkl\n",
      "data\\20250510_185507\\batch_14.pkl\n",
      "data\\20250510_185507\\batch_15.pkl\n",
      "data\\20250510_185507\\batch_16.pkl\n",
      "data\\20250510_185507\\batch_17.pkl\n",
      "data\\20250510_185507\\batch_18.pkl\n",
      "data\\20250510_185507\\batch_19.pkl\n",
      "data\\20250510_185507\\batch_2.pkl\n",
      "data\\20250510_185507\\batch_20.pkl\n",
      "data\\20250510_185507\\batch_21.pkl\n",
      "data\\20250510_185507\\batch_22.pkl\n",
      "data\\20250510_185507\\batch_23.pkl\n",
      "data\\20250510_185507\\batch_24.pkl\n",
      "data\\20250510_185507\\batch_25.pkl\n",
      "data\\20250510_185507\\batch_26.pkl\n",
      "data\\20250510_185507\\batch_27.pkl\n",
      "data\\20250510_185507\\batch_28.pkl\n",
      "data\\20250510_185507\\batch_29.pkl\n",
      "data\\20250510_185507\\batch_3.pkl\n",
      "data\\20250510_185507\\batch_30.pkl\n",
      "data\\20250510_185507\\batch_31.pkl\n",
      "data\\20250510_185507\\batch_32.pkl\n",
      "data\\20250510_185507\\batch_33.pkl\n",
      "data\\20250510_185507\\batch_34.pkl\n",
      "data\\20250510_185507\\batch_35.pkl\n",
      "data\\20250510_185507\\batch_36.pkl\n",
      "data\\20250510_185507\\batch_37.pkl\n",
      "data\\20250510_185507\\batch_38.pkl\n",
      "data\\20250510_185507\\batch_39.pkl\n",
      "data\\20250510_185507\\batch_4.pkl\n",
      "data\\20250510_185507\\batch_40.pkl\n",
      "data\\20250510_185507\\batch_41.pkl\n",
      "data\\20250510_185507\\batch_42.pkl\n",
      "data\\20250510_185507\\batch_43.pkl\n",
      "data\\20250510_185507\\batch_44.pkl\n",
      "data\\20250510_185507\\batch_45.pkl\n",
      "data\\20250510_185507\\batch_46.pkl\n",
      "data\\20250510_185507\\batch_47.pkl\n",
      "data\\20250510_185507\\batch_48.pkl\n",
      "data\\20250510_185507\\batch_49.pkl\n",
      "data\\20250510_185507\\batch_5.pkl\n",
      "data\\20250510_185507\\batch_50.pkl\n",
      "data\\20250510_185507\\batch_51.pkl\n",
      "data\\20250510_185507\\batch_52.pkl\n",
      "data\\20250510_185507\\batch_53.pkl\n",
      "data\\20250510_185507\\batch_54.pkl\n",
      "data\\20250510_185507\\batch_55.pkl\n",
      "data\\20250510_185507\\batch_56.pkl\n",
      "data\\20250510_185507\\batch_57.pkl\n",
      "data\\20250510_185507\\batch_58.pkl\n",
      "data\\20250510_185507\\batch_59.pkl\n",
      "data\\20250510_185507\\batch_6.pkl\n",
      "data\\20250510_185507\\batch_60.pkl\n",
      "data\\20250510_185507\\batch_7.pkl\n",
      "data\\20250510_185507\\batch_8.pkl\n",
      "data\\20250510_185507\\batch_9.pkl\n",
      "data\\20250514_152839\\batch_0.pkl\n",
      "data\\20250514_152839\\batch_1.pkl\n",
      "data\\20250514_152839\\batch_10.pkl\n",
      "data\\20250514_152839\\batch_11.pkl\n",
      "data\\20250514_152839\\batch_12.pkl\n",
      "data\\20250514_152839\\batch_13.pkl\n",
      "data\\20250514_152839\\batch_14.pkl\n",
      "data\\20250514_152839\\batch_15.pkl\n",
      "data\\20250514_152839\\batch_16.pkl\n",
      "data\\20250514_152839\\batch_17.pkl\n",
      "data\\20250514_152839\\batch_18.pkl\n",
      "data\\20250514_152839\\batch_19.pkl\n",
      "data\\20250514_152839\\batch_2.pkl\n",
      "data\\20250514_152839\\batch_20.pkl\n",
      "data\\20250514_152839\\batch_21.pkl\n",
      "data\\20250514_152839\\batch_22.pkl\n",
      "data\\20250514_152839\\batch_23.pkl\n",
      "data\\20250514_152839\\batch_24.pkl\n",
      "data\\20250514_152839\\batch_25.pkl\n",
      "data\\20250514_152839\\batch_26.pkl\n",
      "data\\20250514_152839\\batch_27.pkl\n",
      "data\\20250514_152839\\batch_28.pkl\n",
      "data\\20250514_152839\\batch_29.pkl\n",
      "data\\20250514_152839\\batch_3.pkl\n",
      "data\\20250514_152839\\batch_30.pkl\n",
      "data\\20250514_152839\\batch_31.pkl\n",
      "data\\20250514_152839\\batch_32.pkl\n",
      "data\\20250514_152839\\batch_33.pkl\n",
      "data\\20250514_152839\\batch_34.pkl\n",
      "data\\20250514_152839\\batch_35.pkl\n",
      "data\\20250514_152839\\batch_36.pkl\n",
      "data\\20250514_152839\\batch_37.pkl\n",
      "data\\20250514_152839\\batch_38.pkl\n",
      "data\\20250514_152839\\batch_39.pkl\n",
      "data\\20250514_152839\\batch_4.pkl\n",
      "data\\20250514_152839\\batch_40.pkl\n",
      "data\\20250514_152839\\batch_41.pkl\n",
      "data\\20250514_152839\\batch_42.pkl\n",
      "data\\20250514_152839\\batch_43.pkl\n",
      "data\\20250514_152839\\batch_44.pkl\n",
      "data\\20250514_152839\\batch_45.pkl\n",
      "data\\20250514_152839\\batch_46.pkl\n",
      "data\\20250514_152839\\batch_47.pkl\n",
      "data\\20250514_152839\\batch_48.pkl\n",
      "data\\20250514_152839\\batch_49.pkl\n",
      "data\\20250514_152839\\batch_5.pkl\n",
      "data\\20250514_152839\\batch_50.pkl\n",
      "data\\20250514_152839\\batch_51.pkl\n",
      "data\\20250514_152839\\batch_52.pkl\n",
      "data\\20250514_152839\\batch_53.pkl\n",
      "data\\20250514_152839\\batch_54.pkl\n",
      "data\\20250514_152839\\batch_55.pkl\n",
      "data\\20250514_152839\\batch_56.pkl\n",
      "data\\20250514_152839\\batch_57.pkl\n",
      "data\\20250514_152839\\batch_58.pkl\n",
      "data\\20250514_152839\\batch_59.pkl\n",
      "data\\20250514_152839\\batch_6.pkl\n",
      "data\\20250514_152839\\batch_60.pkl\n",
      "data\\20250514_152839\\batch_61.pkl\n",
      "data\\20250514_152839\\batch_62.pkl\n",
      "data\\20250514_152839\\batch_63.pkl\n",
      "data\\20250514_152839\\batch_64.pkl\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xc0 in position 1: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mUnicodeDecodeError\u001B[39m                        Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 12\u001B[39m\n\u001B[32m     10\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m (\u001B[38;5;28mopen\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mrb\u001B[39m\u001B[33m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m file):\n\u001B[32m     11\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m12\u001B[39m         content = pickle.load(file)\n\u001B[32m     13\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mEOFError\u001B[39;00m:\n\u001B[32m     14\u001B[39m         \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mData is corrupted. Trying to partially recover data...\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mUnicodeDecodeError\u001B[39m: 'utf-8' codec can't decode byte 0xc0 in position 1: invalid start byte"
     ]
    }
   ],
   "source": [
    "# Gather Data from different files\n",
    "print(\"Gathering data...\")\n",
    "data = []\n",
    "dirs = [\"./data/20250512_165251\", \"./data/20250511_095828\", \"./data/20250512_012245\", \"./data/20250510_185507\", \"./data/20250514_152839\"]\n",
    "\n",
    "for dir in dirs:\n",
    "    files = Path(dir).glob(\"*.pkl\")\n",
    "    for file_name in files:\n",
    "        print(file_name, sep=\", \")\n",
    "        with (open(f'{file_name}', 'rb') as file):\n",
    "            try:\n",
    "                content = pickle.load(file)\n",
    "            except EOFError:\n",
    "                print(\"Data is corrupted. Trying to partially recover data...\")\n",
    "                content = safe_unpickle_all(file)\n",
    "                print(content)\n",
    "\n",
    "            data = data + content"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-18T10:02:16.736974500Z",
     "start_time": "2025-05-18T09:58:45.107742100Z"
    }
   },
   "id": "51949d32f8799ddd",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_results = []\n",
    "y_tests = []\n",
    "p_tests = []\n",
    "eces = []\n",
    "sample_sizes = []\n",
    "accuracies = []\n",
    "y = []\n",
    "optimal_eces = []\n",
    "for result in data:\n",
    "    model_result = result[\"model_results\"]\n",
    "    model_results = model_results + model_result\n",
    "    \n",
    "    for r in model_result:\n",
    "        p_tests.append(r[\"p_test\"])\n",
    "        eces.append(r[\"ECEs\"])\n",
    "        accuracies.append(r[\"Accuracy\"])\n",
    "        y.append(r[\"Optimal Sample Size\"])\n",
    "        optimal_eces.append(r[\"Optimal ECE\"])\n",
    "\n",
    "    for i in range(4):\n",
    "        y_tests.append(result[\"y_test\"])\n",
    "        sample_sizes.append(result[\"Sample Sizes\"])\n",
    "\n",
    "# Ensure numpy\n",
    "y_tests = np.array(y_tests)\n",
    "eces = np.array(eces)\n",
    "p_tests = np.array(p_tests, dtype=np.float32)\n",
    "sample_sizes = np.array(sample_sizes)\n",
    "accuracies = np.array(accuracies)\n",
    "y = np.array(y)\n",
    "optimal_eces = np.array(optimal_eces)\n",
    "\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "std_dev_accuracy = np.std(accuracies)\n",
    "\n",
    "dist_from_eces0 = np.array([np.linalg.norm(eces[0] - eces[i]) for i in range(len(eces)) if i != 0])"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "e6723eff4e6e3d7b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(\"y_tests shape\", y_tests.shape)\n",
    "print(\"sample sizes shape\", sample_sizes.shape)\n",
    "print(\"p_tests shape\", p_tests.shape)\n",
    "print(\"eces shape\", eces.shape)\n",
    "print(\"Mean Accuracy\", mean_accuracy)\n",
    "print(\"Std. Dev. Accuracy\", std_dev_accuracy)\n",
    "print(\"Distances from eces[0]\", dist_from_eces0)\n",
    "print(\"Mean distance from eces[0]\", np.mean(dist_from_eces0))\n",
    "print(\"Std. Dev distance from eces[0]\", np.std(dist_from_eces0))\n",
    "\n",
    "print(\"Mean True ECE\", np.mean(np.array([result[\"True ECE Dists (Binned - 15 Bins)\"] for result in model_results])))\n",
    "print(\"Std. Dev True ECE\", np.std(np.array([result[\"True ECE Dists (Binned - 15 Bins)\"] for result in model_results])))\n",
    "\n",
    "print(sample_sizes)\n",
    "print(eces)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-18T10:02:16.567973900Z"
    }
   },
   "id": "8a5cc628353721db",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Prepare Data\n",
    "X = np.hstack((sample_sizes, eces, accuracies.reshape(-1, 1)))\n",
    "y = np.column_stack((y, optimal_eces))\n",
    "\n",
    "print(X)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-18T10:02:16.572971700Z"
    }
   },
   "id": "4bcce1c39a507fbf",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "optimal_eces_train = y_train[:, 1]\n",
    "y_train = y_train[:, 0]\n",
    "optimal_eces_test = y_test[:, 1]\n",
    "y_test = y_test[:, 0]\n",
    "\n",
    "X_train = X_train[:, 100:]\n",
    "X_test = X_test[:, 100:]\n",
    "\n",
    "print(\"X_train Shape\", X_train.shape)\n",
    "print(\"X_test Shape\", X_test.shape)\n",
    "\n",
    "print(\"y_train Shape\", y_train.shape)\n",
    "print(\"y_test Shape\", y_test.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-18T10:02:16.577971800Z"
    }
   },
   "id": "b4b4ff5652a94f31",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Perform Bayes Optimization\n",
    "tuner = keras_tuner.BayesianOptimization(\n",
    "    hypermodel=build_nn,\n",
    "    objective=\"val_loss\",\n",
    "    max_trials=24,\n",
    "    executions_per_trial=1,\n",
    "    directory=\"keras_tuner_logs_bayes_logcosh_dropout_only_ece_values_ext\",\n",
    "    project_name=\"ece_neural_network_bayes2_logscosh_dropout_only_ece_values_ext\",\n",
    ")\n",
    "tuner.search(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=150,\n",
    "    batch_size=128,\n",
    "    callbacks=[keras.callbacks.EarlyStopping(patience=10)],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "y_pred_scaled = np.array(best_model.predict(X_test)).flatten()\n",
    "y_pred = y_pred_scaled\n",
    "print(\"y_pred_scaled NN\", y_pred_scaled)\n",
    "\n",
    "# Print best Hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"Best Hyperparameters:\")\n",
    "for key in best_hps.values.keys():\n",
    "    print(f\"{key}: {best_hps.get(key)}\")\n",
    "\n",
    "\"\"\"\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten())\n",
    "for i in range(4):\n",
    "    model.add(\n",
    "        Dense(\n",
    "            units=480,\n",
    "            activation=\"relu\",\n",
    "        )\n",
    "    )\n",
    "model.add(Dense(1, activation=\"linear\"))\n",
    "\n",
    "learning_rate = 0.0022035143136497045\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    loss=keras.losses.LogCosh(),\n",
    "    metrics=[\"mae\"],\n",
    ")\n",
    "\n",
    "model.fit(X_train_scaled, y_train_scaled)\n",
    "y_pred = y_scaler.inverse_transform(np.array(model.predict(X_test_scaled)).flatten().reshape(-1, 1)).ravel()\n",
    "\"\"\"\n",
    "\n",
    "print(\"y_pred NN\", y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-18T10:02:16.580976400Z"
    }
   },
   "id": "626215944816b53d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Train Regressor\n",
    "model = XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    max_depth=16,\n",
    "    learning_rate=0.002,\n",
    "    subsample=0.78,\n",
    "    colsample_bytree=0.75,\n",
    "    gamma=1,\n",
    "    reg_alpha=0.15,\n",
    "    reg_lambda=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbosity=1\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "y_pred_scaled_regressor = np.array(model.predict(X_test)).flatten()\n",
    "y_pred_regressor = y_pred_scaled_regressor\n",
    "print(\"y_pred_scaled regressor\", y_pred_scaled_regressor)\n",
    "print(\"y_pred_regressor\", y_pred_regressor)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-18T10:02:16.583971800Z"
    }
   },
   "id": "425921406125bf1c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Calculate Metrics and display Dataframe\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mX_test\u001B[39m\u001B[33m\"\u001B[39m, \u001B[43mX_test\u001B[49m)\n\u001B[32m      3\u001B[39m X_test_eces = X_test[:, :\u001B[32m100\u001B[39m]\n\u001B[32m      4\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mX_test_eces\u001B[39m\u001B[33m\"\u001B[39m, X_test_eces)\n",
      "\u001B[31mNameError\u001B[39m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "# Calculate Metrics and display Dataframe\n",
    "print(\"X_test\", X_test)\n",
    "X_test_eces = X_test[:, :100]\n",
    "print(\"X_test_eces\", X_test_eces)\n",
    "print(\"Sample Sizes\", sample_sizes)\n",
    "\n",
    "\n",
    "simple_strategy_preds = [\n",
    "    (f\"{(i + 1) * 100} Samples\", (sample_sizes[len(X_train):, i], X_test_eces[:, i])) for i in [0, 4, 9, 19, 49, 79, 99]]\n",
    "\n",
    "def get_rounded_clipped_sample_size(sample_size):\n",
    "    return max(15, min(10000, round(sample_size)))\n",
    "\n",
    "preds = {}\n",
    "for pred in simple_strategy_preds:\n",
    "    preds[pred[0]] = pred[1]\n",
    "    \n",
    "\n",
    "num_ece_subsets = 20\n",
    "\n",
    "\n",
    "ece_nn_interpolated = np.array([interpolate_indexed_value_array(sample_sizes[i], X_test_eces[i], sample_size) for\n",
    "                                (i, sample_size) in enumerate(y_pred)])\n",
    "\n",
    "\n",
    "ece_regressor_interpolated = np.array([interpolate_indexed_value_array(sample_sizes[i], X_test_eces[i], sample_size) for\n",
    "                      (i, sample_size) in enumerate(y_pred_regressor)])\n",
    "\n",
    "ece_nn_interpolated_quad = np.array([interpolate_indexed_value_array(sample_sizes[i], X_test_eces[i], sample_size, strategy=\"quadratic\") for\n",
    "                      (i, sample_size) in enumerate(y_pred)])\n",
    "\n",
    "ece_regressor_interpolated_quad = np.array([interpolate_indexed_value_array(sample_sizes[i], X_test_eces[i], sample_size, strategy=\"quadratic\") for\n",
    "                                      (i, sample_size) in enumerate(y_pred_regressor)])\n",
    "preds.update({\n",
    "    \"Neural Network\": (\n",
    "        y_pred, np.array([ece(p_tests[i][:get_rounded_clipped_sample_size(sample_size)], y_tests[i][:get_rounded_clipped_sample_size(sample_size)], n_bins=15) for\n",
    "                          i, sample_size in enumerate(y_pred)])),\n",
    "    \"XGBRegressor\": (y_pred_regressor,\n",
    "                     np.array([ece(p_tests[i][:get_rounded_clipped_sample_size(sample_size)], y_tests[i][:get_rounded_clipped_sample_size(sample_size)], n_bins=15) for\n",
    "                               i, sample_size in enumerate(y_pred_regressor)])),\n",
    "    f\"Neural Network (ECE on subsets {num_ece_subsets}\": (\n",
    "        y_pred, np.array([np.mean(calculate_ece_on_subsets(p_tests[i], y_tests[i], sample_size, num_ece_subsets)) for i, sample_size in enumerate(y_pred)])),\n",
    "    f\"XGBRegressor (ECE on subsets {num_ece_subsets}\": (\n",
    "        y_pred_regressor, np.array([np.mean(calculate_ece_on_subsets(p_tests[i], y_tests[i], sample_size, num_ece_subsets)) for i, sample_size in enumerate(y_pred_regressor)])),\n",
    "    \"Neural Network (Interpolated)\": (y_pred, ece_nn_interpolated),\n",
    "    \"XGBRegressor (Interpolated)\": (y_pred_regressor, ece_regressor_interpolated),\n",
    "    \"Interpolated Average\": ((y_pred + y_pred_regressor)/ 2, (ece_nn_interpolated + ece_regressor_interpolated) / 2),\n",
    "    \"Neural Network (Interpolated - Quadratic)\": (y_pred, ece_nn_interpolated_quad),\n",
    "    \"XGBRegressor (Interpolated - Quadratic)\": (y_pred_regressor, ece_regressor_interpolated_quad),\n",
    "    \"Interpolated (Quadratic) Average\": ((y_pred + y_pred_regressor) / 2, (ece_nn_interpolated_quad + ece_regressor_interpolated_quad) / 2),\n",
    "    \"Y_Test\": (y_test, optimal_eces_test)\n",
    "})\n",
    "\n",
    "print(\"Optimal ECEs\", optimal_eces_test)\n",
    "print(\"y_test\", y_test)\n",
    "# Compute stats\n",
    "results = []\n",
    "for name, (sample_sizes_, eces) in preds.items():\n",
    "    print(name, sample_sizes_, eces)\n",
    "    results.append({\n",
    "        \"Name\": name,\n",
    "        \"Mean (Samples)\": np.mean(sample_sizes_),\n",
    "        \"Std Dev (Samples)\": np.std(sample_sizes_),\n",
    "        \"MSE (Samples)\": mean_squared_error(y_test, sample_sizes_),\n",
    "        \"MAE (Samples)\": mean_absolute_error(y_test, sample_sizes_),\n",
    "        \"R2-Score (Samples)\": r2_score(y_test, sample_sizes_),\n",
    "        \"Mean (ECE)\": np.mean(eces),\n",
    "        \"Std Dev (ECE)\": np.std(eces),\n",
    "        \"MSE (ECE)\": mean_squared_error(optimal_eces_test, eces),\n",
    "        \"MAE (ECE)\": mean_absolute_error(optimal_eces_test, eces),\n",
    "        \"R2-Score (ECE)\": r2_score(optimal_eces_test, eces),\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "pd.set_option('display.max_columns', None)\n",
    "df = pd.DataFrame(results)\n",
    "print(df) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-21T12:30:12.365202400Z",
     "start_time": "2025-05-21T12:30:12.295263600Z"
    }
   },
   "id": "e0706e7769c701e7",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "num_preds = len(preds)\n",
    "cols = 4  # You can change this\n",
    "rows = int(np.ceil(num_preds / cols))\n",
    "\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(20, 4 * rows), constrained_layout=True)\n",
    "\n",
    "# Flatten axes array for easy indexing\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Plot each prediction vs. optimal\n",
    "for i, (key, value) in enumerate(preds.items()):\n",
    "    ax = axs[i]\n",
    "    if len(value) > 1:\n",
    "        ax.hist(optimal_eces_test, bins=50, alpha=0.5, label=\"Optimal ECEs\")\n",
    "        ax.hist(value[1], bins=50, alpha=0.5, label=key)\n",
    "        ax.set_title(f\"{key} vs Optimal\")\n",
    "        ax.set_xlabel(\"ECE\")\n",
    "        ax.set_ylabel(\"Frequency\")\n",
    "        ax.legend()\n",
    "\n",
    "# Hide unused subplots if any\n",
    "for j in range(i + 1, len(axs)):\n",
    "    fig.delaxes(axs[j])\n",
    "\n",
    "plt.suptitle(\"Target vs Prediction Distributions\", fontsize=16)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-18T10:02:16.587971900Z"
    }
   },
   "id": "7602d9848f69654",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "num_preds = len(preds)\n",
    "cols = 4 \n",
    "rows = int(np.ceil(num_preds / cols))\n",
    "\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(20, 4 * rows), constrained_layout=True)\n",
    "axs = axs.flatten()\n",
    "\n",
    "for i, (key, value) in enumerate(preds.items()):\n",
    "    ax = axs[i]\n",
    "    if len(value) > 1:\n",
    "        ax.scatter(value[1], optimal_eces_test)\n",
    "        ax.plot(optimal_eces_test, optimal_eces_test, c='red')\n",
    "        ax.set_title(f\"{key} vs Optimal\")\n",
    "        ax.set_xlabel(\"Prediction\")\n",
    "        ax.set_ylabel(\"Optimal ECE\")\n",
    "\n",
    "# Hide unused subplots if any\n",
    "for j in range(i + 1, len(axs)):\n",
    "    fig.delaxes(axs[j])\n",
    "\n",
    "plt.suptitle(\"Target vs Prediction Plot\", fontsize=16)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-18T10:02:16.589972100Z"
    }
   },
   "id": "97733e6cf7ba5c44",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-18T10:02:16.591971700Z"
    }
   },
   "id": "ae2b98918154efa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
